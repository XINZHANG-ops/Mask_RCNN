{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Base Configuration Class\n",
    "# Don't use this class directly. Instead, sub-class it and override\n",
    "# the configurations you need to change.\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Base configuration class. For custom configurations, create a\n",
    "    sub-class that inherits from this one and override properties\n",
    "    that need to be changed.\n",
    "    \"\"\"\n",
    "    # Name the configurations. For example, 'COCO', 'Experiment 3', ...etc.\n",
    "    # Useful if your code needs to do things differently depending on which\n",
    "    # experiment is running.\n",
    "    NAME = None  # Override in sub-classes\n",
    "\n",
    "    # NUMBER OF GPUs to use. When using only a CPU, this needs to be set to 1.\n",
    "    GPU_COUNT = 1\n",
    "\n",
    "    # Number of images to train with on each GPU. A 12GB GPU can typically\n",
    "    # handle 2 images of 1024x1024px.\n",
    "    # Adjust based on your GPU memory and image sizes. Use the highest\n",
    "    # number that your GPU can handle for best performance.\n",
    "    IMAGES_PER_GPU = 2\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    # This doesn't need to match the size of the training set. Tensorboard\n",
    "    # updates are saved at the end of each epoch, so setting this to a\n",
    "    # smaller number means getting more frequent TensorBoard updates.\n",
    "    # Validation stats are also calculated at each epoch end and they\n",
    "    # might take a while, so don't set this too small to avoid spending\n",
    "    # a lot of time on validation stats.\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "\n",
    "    # Number of validation steps to run at the end of every training epoch.\n",
    "    # A bigger number improves accuracy of validation stats, but slows\n",
    "    # down the training.\n",
    "    VALIDATION_STEPS = 50\n",
    "\n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101.\n",
    "    # You can also provide a callable that should have the signature\n",
    "    # of model.resnet_graph. If you do so, you need to supply a callable\n",
    "    # to COMPUTE_BACKBONE_SHAPE as well\n",
    "    BACKBONE = \"resnet101\"\n",
    "\n",
    "    # Only useful if you supply a callable to BACKBONE. Should compute\n",
    "    # the shape of each layer of the FPN Pyramid.\n",
    "    # See model.compute_backbone_shapes\n",
    "    COMPUTE_BACKBONE_SHAPE = None\n",
    "\n",
    "    # The strides of each layer of the FPN Pyramid. These values\n",
    "    # are based on a Resnet101 backbone.\n",
    "    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "\n",
    "    # Size of the fully-connected layers in the classification graph\n",
    "    FPN_CLASSIF_FC_LAYERS_SIZE = 1024\n",
    "\n",
    "    # Size of the top-down layers used to build the feature pyramid\n",
    "    TOP_DOWN_PYRAMID_SIZE = 256\n",
    "\n",
    "    # Number of classification classes (including background)\n",
    "    NUM_CLASSES = 1  # Override in sub-classes\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "\n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "\n",
    "    # Anchor stride\n",
    "    # If 1 then anchors are created for each cell in the backbone feature map.\n",
    "    # If 2, then anchors are created for every other cell, and so on.\n",
    "    RPN_ANCHOR_STRIDE = 1\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.7\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 256\n",
    "    \n",
    "    # ROIs kept after tf.nn.top_k and before non-maximum suppression\n",
    "    PRE_NMS_LIMIT = 6000\n",
    "\n",
    "    # ROIs kept after non-maximum suppression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 2000\n",
    "    POST_NMS_ROIS_INFERENCE = 1000\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "\n",
    "    # Input image resizing\n",
    "    # Generally, use the \"square\" resizing mode for training and predicting\n",
    "    # and it should work well in most cases. In this mode, images are scaled\n",
    "    # up such that the small side is = IMAGE_MIN_DIM, but ensuring that the\n",
    "    # scaling doesn't make the long side > IMAGE_MAX_DIM. Then the image is\n",
    "    # padded with zeros to make it a square so multiple images can be put\n",
    "    # in one batch.\n",
    "    # Available resizing modes:\n",
    "    # none:   No resizing or padding. Return the image unchanged.\n",
    "    # square: Resize and pad with zeros to get a square image\n",
    "    #         of size [max_dim, max_dim].\n",
    "    # pad64:  Pads width and height with zeros to make them multiples of 64.\n",
    "    #         If IMAGE_MIN_DIM or IMAGE_MIN_SCALE are not None, then it scales\n",
    "    #         up before padding. IMAGE_MAX_DIM is ignored in this mode.\n",
    "    #         The multiple of 64 is needed to ensure smooth scaling of feature\n",
    "    #         maps up and down the 6 levels of the FPN pyramid (2**6=64).\n",
    "    # crop:   Picks random crops from the image. First, scales the image based\n",
    "    #         on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of\n",
    "    #         size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only.\n",
    "    #         IMAGE_MAX_DIM is not used in this mode.\n",
    "    IMAGE_RESIZE_MODE = \"square\"\n",
    "    IMAGE_MIN_DIM = 800\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    # Minimum scaling ratio. Checked after MIN_IMAGE_DIM and can force further\n",
    "    # up scaling. For example, if set to 2 then images are scaled up to double\n",
    "    # the width and height, or more, even if MIN_IMAGE_DIM doesn't require it.\n",
    "    # However, in 'square' mode, it can be overruled by IMAGE_MAX_DIM.\n",
    "    IMAGE_MIN_SCALE = 0\n",
    "    # Number of color channels per image. RGB = 3, grayscale = 1, RGB-D = 4\n",
    "    # Changing this requires other changes in the code. See the WIKI for more\n",
    "    # details: https://github.com/matterport/Mask_RCNN/wiki\n",
    "    IMAGE_CHANNEL_COUNT = 3\n",
    "\n",
    "    # Image mean (RGB)\n",
    "    MEAN_PIXEL = np.array([123.7, 116.8, 103.9])\n",
    "\n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # the RPN NMS threshold.\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "\n",
    "    # Percent of positive ROIs used to train classifier/mask heads\n",
    "    ROI_POSITIVE_RATIO = 0.33\n",
    "\n",
    "    # Pooled ROIs\n",
    "    POOL_SIZE = 7\n",
    "    MASK_POOL_SIZE = 14\n",
    "\n",
    "    # Shape of output mask\n",
    "    # To change this you also need to change the neural network mask branch\n",
    "    MASK_SHAPE = [28, 28]\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 100\n",
    "\n",
    "    # Bounding box refinement standard deviation for RPN and final detections.\n",
    "    RPN_BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "    BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "\n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 100\n",
    "\n",
    "    # Minimum probability value to accept a detected instance\n",
    "    # ROIs below this threshold are skipped\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7\n",
    "\n",
    "    # Non-maximum suppression threshold for detection\n",
    "    DETECTION_NMS_THRESHOLD = 0.3\n",
    "\n",
    "    # Learning rate and momentum\n",
    "    # The Mask RCNN paper uses lr=0.02, but on TensorFlow it causes\n",
    "    # weights to explode. Likely due to differences in optimizer\n",
    "    # implementation.\n",
    "    LEARNING_RATE = 0.001\n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "\n",
    "    # Weight decay regularization\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "\n",
    "    # Loss weights for more precise optimization.\n",
    "    # Can be used for R-CNN training setup.\n",
    "    LOSS_WEIGHTS = {\n",
    "        \"rpn_class_loss\": 1.,\n",
    "        \"rpn_bbox_loss\": 1.,\n",
    "        \"mrcnn_class_loss\": 1.,\n",
    "        \"mrcnn_bbox_loss\": 1.,\n",
    "        \"mrcnn_mask_loss\": 1.\n",
    "    }\n",
    "\n",
    "    # Use RPN ROIs or externally generated ROIs for training\n",
    "    # Keep this True for most situations. Set to False if you want to train\n",
    "    # the head branches on ROI generated by code rather than the ROIs from\n",
    "    # the RPN. For example, to debug the classifier head without having to\n",
    "    # train the RPN.\n",
    "    USE_RPN_ROIS = True\n",
    "\n",
    "    # Train or freeze batch normalization layers\n",
    "    #     None: Train BN layers. This is the normal mode\n",
    "    #     False: Freeze BN layers. Good when using a small batch size\n",
    "    #     True: (don't use). Set layer in training mode even when predicting\n",
    "    TRAIN_BN = False  # Defaulting to False since batch size is often small\n",
    "\n",
    "    # Gradient norm clipping\n",
    "    GRADIENT_CLIP_NORM = 5.0\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Set values of computed attributes.\"\"\"\n",
    "        # Effective batch size\n",
    "        self.BATCH_SIZE = self.IMAGES_PER_GPU * self.GPU_COUNT\n",
    "\n",
    "        # Input image size\n",
    "        if self.IMAGE_RESIZE_MODE == \"crop\":\n",
    "            self.IMAGE_SHAPE = np.array([self.IMAGE_MIN_DIM, self.IMAGE_MIN_DIM,\n",
    "                self.IMAGE_CHANNEL_COUNT])\n",
    "        else:\n",
    "            self.IMAGE_SHAPE = np.array([self.IMAGE_MAX_DIM, self.IMAGE_MAX_DIM,\n",
    "                self.IMAGE_CHANNEL_COUNT])\n",
    "\n",
    "        # Image meta data length\n",
    "        # See compose_image_meta() for details\n",
    "        self.IMAGE_META_SIZE = 1 + 3 + 3 + 4 + 1 + self.NUM_CLASSES\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display Configuration values.\"\"\"\n",
    "        print(\"\\nConfigurations:\")\n",
    "        for a in dir(self):\n",
    "            if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
    "                print(\"{:30} {}\".format(a, getattr(self, a)))\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-multiple",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-perform-object-detection-in-photographs-with-mask-r-cnn-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-president",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --no-deps keras==2.2.4\n",
    "# pip install tensorflow==1.15.3\n",
    "# pip install h5py==2.10.0\n",
    "# pip install scikit-image==0.16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestConfig(Config):\n",
    "     NAME = \"test\"\n",
    "     GPU_COUNT = 1\n",
    "     IMAGES_PER_GPU = 1\n",
    "     NUM_CLASSES = 1 + 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.model import MaskRCNN\n",
    "# define the model\n",
    "rcnn = MaskRCNN(mode='inference', model_dir='./', config=TestConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load coco model weights\n",
    "rcnn.load_weights('mask_rcnn_coco.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of inference with a pre-trained coco model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    " \n",
    "# draw an image with detected objects\n",
    "def draw_image_with_boxes(filename, boxes_list):\n",
    "     # load the image\n",
    "     data = pyplot.imread(filename)\n",
    "     # plot the image\n",
    "     pyplot.imshow(data)\n",
    "     # get the context for drawing boxes\n",
    "     ax = pyplot.gca()\n",
    "     # plot each box\n",
    "     for box in boxes_list:\n",
    "          # get coordinates\n",
    "          y1, x1, y2, x2 = box\n",
    "          # calculate width and height of the box\n",
    "          width, height = x2 - x1, y2 - y1\n",
    "          # create the shape\n",
    "          rect = Rectangle((x1, y1), width, height, fill=False, color='red')\n",
    "          # draw the box\n",
    "          ax.add_patch(rect)\n",
    "     # show the plot\n",
    "     pyplot.show()\n",
    " \n",
    "# define the test configuration\n",
    "class TestConfig(Config):\n",
    "     NAME = \"test\"\n",
    "     GPU_COUNT = 1\n",
    "     IMAGES_PER_GPU = 1\n",
    "     NUM_CLASSES = 1 + 80\n",
    " \n",
    "# define the model\n",
    "rcnn = MaskRCNN(mode='inference', model_dir='./', config=TestConfig())\n",
    "# load coco model weights\n",
    "rcnn.load_weights('mask_rcnn_coco.h5', by_name=True)\n",
    "# load photograph\n",
    "img = load_img('elephant.jpg')\n",
    "img = img_to_array(img)\n",
    "# make prediction\n",
    "results = rcnn.detect([img], verbose=0)\n",
    "# visualize the results\n",
    "draw_image_with_boxes('elephant.jpg', results[0]['rois'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of inference with a pre-trained coco model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from mrcnn.visualize import display_instances\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "\n",
    "# define 81 classes that the coco model knowns about\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "               'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "# define the test configuration\n",
    "class TestConfig(Config):\n",
    "     NAME = \"test\"\n",
    "     GPU_COUNT = 1\n",
    "     IMAGES_PER_GPU = 1\n",
    "     NUM_CLASSES = 1 + 80\n",
    "\n",
    "# define the model\n",
    "rcnn = MaskRCNN(mode='inference', model_dir='./', config=TestConfig())\n",
    "# load coco model weights\n",
    "rcnn.load_weights('mask_rcnn_coco.h5', by_name=True)\n",
    "# load photograph\n",
    "img = load_img('elephant.jpg')\n",
    "img = img_to_array(img)\n",
    "# make prediction\n",
    "results = rcnn.detect([img], verbose=0)\n",
    "# get dictionary for first prediction\n",
    "r = results[0]\n",
    "# show photo with bounding boxes, masks, class labels and scores\n",
    "display_instances(img, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-avenue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-solution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-cancellation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-latin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "rapids-gpu.0-18.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/rapids-gpu.0-18:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
